<!DOCTYPE html>
<html lang="en">

<head>
    
<meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
<meta name="HandheldFriendly" content="True" />
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
<meta name="generator" content="Hugo 0.112.3">


<link rel="shortcut icon" href="https://cdn.jsdelivr.net/gh/dsrkafuu/dsr-cdn-main@1/images/favicons/dsrca.ico" />



<title>Python NLP - NLTK and scikit-learn - My Writings - Bill Chambers</title>


<meta name="author" content="Bill Chambers" />


<meta name="description" content="something" />



<meta property="og:title" content="Python NLP - NLTK and scikit-learn" />
<meta name="twitter:title" content="Python NLP - NLTK and scikit-learn" />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://billchambers.me/posts/python-nlp-cheatsheet-nltk-scikit-learn/" /><meta property="og:description" content="This post is meant as a summary of many of the concepts that I learned in Marti Hearst&rsquo;s Natural Language Processing class at the UC Berkeley School of Information. I wanted to record the concepts and approaches that I had learned with quick overviews of the code you need to get it working. I figured that it could help some other people get a handle on the goals and code to get things done." />
<meta name="twitter:description" content="This post is meant as a summary of many of the concepts that I learned in Marti Hearst&rsquo;s Natural Language Processing class at the UC Berkeley School of Information. I wanted to record the concepts and approaches that I had learned with quick overviews of the code you need to get it working. I figured that it could help some other people get a handle on the goals and code to get things done." /><meta name="twitter:card" content="summary" /><meta property="article:published_time" content="2015-01-14T00:00:00+00:00" /><meta property="article:modified_time" content="2015-01-14T00:00:00+00:00" />


<style>
    @media (prefers-color-scheme: dark) {
        body[data-theme='auto'] img {
            filter: brightness(60%);
        }
    }

    body[data-theme='dark'] img {
        filter: brightness(60%);
    }
</style>




<link rel="stylesheet" href="http://billchambers.me/assets/css/fuji.min.b4a21b5d3eb1d0a51297e31230a65fc25e387843e45ec3a2d9176cd8d163c216d99b9b13a618b28f537c3b559ec8a408183b0fbfad48daddb9befa7d3ef90eed.css" integrity="sha512-tKIbXT6x0KUSl&#43;MSMKZfwl44eEPkXsOi2Rds2NFjwhbZm5sTphiyj1N8O1WeyKQIGDsPv61I2t25vvp9PvkO7Q==" />








</head>

<body
  data-theme="auto"
  data-theme-auto='false'
  >
    <script data-cfasync="false">
  
  var fujiThemeData = localStorage.getItem('fuji_data-theme');
  
  if (!fujiThemeData) {
    localStorage.setItem('fuji_data-theme', 'auto');
  } else {
    
    if (fujiThemeData !== 'auto') {
      document.body.setAttribute('data-theme', fujiThemeData === 'dark' ? 'dark' : 'light');
    }
  }
</script>

    <header>
    <div class="container-lg clearfix">
        <div class="col-12 header">
            <a class="title-main" href="http://billchambers.me">My Writings - Bill Chambers</a>
            
        </div>
    </div>
</header>

    <main>
        <div class="container-lg clearfix">
            
            <div class="col-12 col-md-9 float-left content">
                
<article>
    
    <h2 class="post-item post-title">
        <a href="http://billchambers.me/posts/python-nlp-cheatsheet-nltk-scikit-learn/">Python NLP - NLTK and scikit-learn</a>
    </h2>
    <div class="post-item post-meta">
        <span><i class="iconfont icon-today-sharp"></i>&nbsp;2015-01-14</span>



<span><i class="iconfont icon-pricetags-sharp"></i>&nbsp;No tag</span>

    </div>
    
    <div class="post-content markdown-body">
        <p>This post is meant as a summary of many of the concepts that I learned in <a href="http://people.ischool.berkeley.edu/%7Ehearst/" target="_blank">Marti Hearst&rsquo;s</a> <a href="http://www.ischool.berkeley.edu/courses/i256" target="_blank">Natural Language Processing</a> class at the <a href="http://www.ischool.berkeley.edu/" target="_blank">UC Berkeley School of Information</a>. I wanted to record the concepts and approaches that I had learned with quick overviews of the code you need to get it working. I figured that it could help some other people get a handle on the goals and code to get things done.</p>
<p><a href="http://www.nltk.org/book_1ed/" target="_blank"><img class="img-zoomable" src="/nlp_processing.gif" alt="Natural Language Processing with Python" />
</a></p>
<p>I would encourage anyone else to take a look at the <a href="http://www.nltk.org/book_1ed/" target="_blank">Natural Language Processing with Python</a> and read more about scikit-learn.</p>
<h4 id="tokenization">Tokenization</h4>
<p>The goal of tokenization is to break up a sentence or paragraph into specific tokens or words. We basically want to convert human language into a more abstract representation that computers can work with.</p>
<p>Sometimes you want to split sentence by sentence and other times you just want to split words.</p>
<p><strong>Sentence Tokenizers</strong></p>
<pre tabindex="0"><code>sent\_tokenizer = nltk.data.load(&#39;tokenizers/punkt/english.pickle&#39;)
</code></pre><p>Here&rsquo;s a popular word regular expression tokenizer from the NLTK book that works quite well.</p>
<p><strong>Word Tokenizers</strong></p>
<pre tabindex="0"><code>tokenization\_pattern = r&#39;&#39;&#39;(?x) # set flag to allow verbose regexps
([A-Z]\.)+ # abbreviations, e.g. U.S.A.
| \w+(-\w+)\* # words with optional internal hyphens
| \$?\d+(\.\d+)?%? # currency and percentages, e.g. $12.40, 82%
| \w+[\x90-\xff] # these are escaped emojis
| [][.,;&#34;&#39;?():-\_`] # these are separate tokens
&#39;&#39;&#39;
word\_tokenizer = nltk.tokenize.regexp.RegexpTokenizer(tokenization\_pattern)
</code></pre><h4 id="part-of-speech-tagging">Part of Speech Tagging</h4>
<p>Once you&rsquo;ve tokenized the sentences you need to tag them. Tagging is not necessary for all purposes but it does help the computer better understand the objects and references in your sentences. Remember our goal is to encode semantics, not words, and tagging can help us do that.</p>
<p>Unfortunately, this is an imperfect science, it&rsquo;s just never going to work out perfectly because in so many sentences there are so many different representations of text. Let me show you what I mean, I&rsquo;ll be using a comical example of a <a href="http://en.wikipedia.org/wiki/Garden_path_sentence" target="_blank">garden path sentence.</a></p>
<p><img class="img-zoomable" src="/discovery-crash-blossom-headline-death-happens-more-slowly-than-thought.jpg" alt="garden path sentence" />
</p>
<p>This sentence is comical because death can either happen more slowly than thought (as in we had an expectation of death happening at a certain rate of speed).</p>
<p>But semantically, the speed of death can compared to the speed of thought which is obviously strange. Once you learn about these kinds of comical sentence structures, you start to seem them more often.</p>
<p><img class="img-zoomable" src="/stan-carey-crash-blossoms-mcdonalds-fries-the-holy-grail-for-potato-farmers1.jpg" alt="garden path sentence" />
</p>
<p>This one is also comical. In this sentence we&rsquo;ve got two meanings as well. McDonald&rsquo;s fries are the holy grail for potato farmers or more comically McDonald&rsquo;s <em>fries</em> the actual holy grail for potato farmers. A comical mental image.</p>
<p><em>images from <a href="https://stancarey.wordpress.com/" target="_blank">Sentence first</a></em></p>
<p>Thus part of speech tagging is never perfect, because there are so many interpretations.</p>
<p><strong>Built in tagger</strong></p>
<p>This is the built in tagger, the one that NLTK recommends. It&rsquo;s pretty slow when working on sort of large corpus.</p>
<pre tabindex="0"><code>nltk.pos\_tag(sentence) # tokenized sentence
nltk.batch\_pos\_tag(sentences) # for lots of tokenized sentences
</code></pre><p><strong>Unigram, Bigram, and Backoff Tagging</strong></p>
<p>These are backoff taggers, basically it&rsquo;s just a dictionary look up to tag parts of speech. You train it on a tagged corpus(or corpora) and then use it to tag sentences in the future.</p>
<pre tabindex="0"><code>default\_tagger = nltk.DefaultTagger(&#39;NN&#39;)
raw = r&#39;&#39;&#39;what will this silly tagger do?&#39;&#39;&#39;
tokens = nltk.word\_tokenize(raw)
print default\_tagger.tag(tokens)
# [(&#39;what&#39;, &#39;NN&#39;), (&#39;will&#39;, &#39;NN&#39;), (&#39;this&#39;, &#39;NN&#39;), (&#39;silly&#39;, &#39;NN&#39;), (&#39;tagger&#39;, &#39;NN&#39;), (&#39;do&#39;, &#39;NN&#39;), (&#39;?&#39;, &#39;NN&#39;)]
</code></pre><p>Here&rsquo;s how you train the tagger on brown, this is a unigram tagger, so it&rsquo;s not going to perform really well because it will tag everything as a NN (noun) or whatever part of speech we give it.</p>
<pre tabindex="0"><code>from nltk.corpus import brown
brown\_tagged\_sents = brown.tagged\_sents()
unigram\_tagger = nltk.UnigramTagger(brown\_tagged\_sents)
print &#34;%0.3f&#34; % unigram\_tagger.evaluate(test\_sents) # eval the tagger
</code></pre><p>This is a true backoff tagger that defaults to a certain part of speech. So it will look for trigram occurrences and see if it finds any with a certain word formation, if it does not then it will <em>backoff</em> to the bigram tagger, etc.</p>
<pre tabindex="0"><code>def build\_backoff\_tagger(train\_sents):
    t0 = nltk.DefaultTagger(&#39;NN&#39;)
    t1 = nltk.UnigramTagger(train\_sents, backoff=t0)
    t2 = nltk.BigramTagger(train\_sents, backoff=t1)
    t3 = nltk.TrigramTagger(train\_sents, backoff=t2)
    return t3
ngram\_tagger = build\_backoff\_tagger(train\_sents)
</code></pre><p>What&rsquo;s nice is to speed things up, you can actually just pickle the backoff tagger so that it&rsquo;s easier to deploy a tagger if need be.</p>
<pre tabindex="0"><code>import pickle # or cPickle
with open(&#39;pickled\_file.pickle&#39;, &#39;wb&#39;) as f:
    pickle.dump(ngram\_tagger, f)
</code></pre><pre tabindex="0"><code>with open(&#39;pickled\_file.pickle&#39;, &#39;r&#39;) as f:
    tagger = pickle.load(f)
</code></pre><h4 id="removing-punctuation">Removing Punctuation</h4>
<p>At times you&rsquo;ll need to remove certain punctuation marks - this is an easy way to do so.</p>
<pre tabindex="0"><code>import string
nopunct = [w for w in text if w not in string.punctuation]
&#39; &#39;.join(nopunct[0:100])
</code></pre><h4 id="stopwords">Stopwords</h4>
<p>Here&rsquo;s an easy way to remove stop words.</p>
<pre tabindex="0"><code>from nltk.corpus import stopwords
normalized = [w for w in text6 if w.lower() not in stopwords.words(&#39;english&#39;)]
</code></pre><p>Extend it with:</p>
<pre tabindex="0"><code>from nltk.corpus import stopwords
my\_stops = stopwords
my\_stops.append(&#34;shoebox&#34;)
</code></pre><h4 id="stemming">Stemming</h4>
<p>Stemming the process by which endings are removed from words in order to remove things like tense or plurality. It&rsquo;s not appropriate for all cases but can make it easier to connect together tenses to see if you&rsquo;re covering the same subject matter.</p>
<p><a href="http://youtu.be/9Zag7uhjdYo?t=30m00s" target="_blank">Ben Hamner mentions in his Machine Learning Best practices that Kaggle has learned from their competitions that the Porter stemmer is consistently used in winning NLP algorithms for their competitions.</a></p>
<pre tabindex="0"><code>pstemmer = nltk.PorterStemmer()
lstemmer = nltk.LancasterStemmer()
wnlemmatizer = nltk.WordNetLemmatizer()
</code></pre><h4 id="frequency-distributions">Frequency Distributions</h4>
<p>A common go to to see what&rsquo;s going on with certain text data sets, frequency distributions allow you to see the frequency at which certain words occur and plot it if need be.</p>
<pre tabindex="0"><code>fd = nltk.FreqDist(data)
fd.plot()
fd.plot(50, cumulative=True)
fd.most\_common(12)
</code></pre><h4 id="collocations-bigrams-trigrams">Collocations, Bigrams, Trigrams</h4>
<p>Bigrams and trigrams are just words that are commonly found together and measures their relevance by a certain measurement.</p>
<pre tabindex="0"><code>bigram\_measures = nltk.collocations.BigramAssocMeasures()
trigram\_measures = nltk.collocations.TrigramAssocMeasures()
finder = nltk.collocations.BigramCollocationFinder.from\_words(text)
finder.nbest(bigram\_measures.pmi, 10)
</code></pre><h4 id="chunking">Chunking</h4>
<p>Chunking basically just grabs chunks of text that might be more meaningful to your research or program. You create a list of parts of speech and run that over your corpus. It will extract the phrasing that you need.</p>
<p>Remember you&rsquo;ve got to customize it to the part of speech tagger that you&rsquo;re using, like Brown or the Stanford Tagger.</p>
<pre tabindex="0"><code>technical\_term = r&#34;T: {&lt;(JJ|NN|NNS|NNP|NNPS)&gt;+&lt;(NN|NNS|NNP|NNPS|CD)&gt;|&lt;(NN|NNS|NNP|NNPS)&gt;}&#34;
cp = nltk.RegexpParser(technical\_term)

for count, sent in enumerate(brown.sents()[100:104]):
    print &#34;Sentence #&#34; + str(count) + &#34;:&#34;
    parsed = cp.parse(nltk.pos\_tag(sent))
    print parsed
    print &#34;\nTechnical Terms:\n&#34;
    for tree in parsed.subtrees():
        if tree.label() == &#34;T&#34;:
            print tree
</code></pre><h4 id="splitting-training-sets--test-sets">Splitting Training Sets + Test Sets</h4>
<p>This is a simple way that Marti showed us that allows for simple splitting of test sets.</p>
<p>This splits it into thirds.</p>
<p><strong>Train, Dev, Test Sets</strong></p>
<pre tabindex="0"><code>def create\_training\_sets\_trips(feature\_function, items):
    featuresets = [(feature\_function(key), value) for (key, value) in items]
    third = int(float(len(featuresets)) / 3.0)
    return items[0:third], items[third:third\*2], items[third\*2:], featuresets[0:third], featuresets[third:third\*2], featuresets[third\*2:]

train\_items, dev\_items, test\_items, train\_features, dev\_features, test\_features = create\_training\_sets\_trips(f\_func, data)
</code></pre><p>This splits it into halves.</p>
<p><strong>Simpler Test Sets</strong></p>
<pre tabindex="0"><code>def create\_training\_sets(feature\_function, items):
    featuresets = [(feature\_function(key), value) for (key, value) in items]
    halfsize = int(float(len(featuresets)) / 2.0)
    train\_set, test\_set = featuresets[halfsize:], featuresets[:halfsize]
    return train\_set, test\_set
train, test = create\_training\_sets(f\_func, data)
</code></pre><h4 id="classifiers--scikit-learn">Classifiers &amp; Scikit-learn</h4>
<p><img class="img-zoomable" src="/scikit-learn-logo-small.png" alt="scikit-learn" />
</p>
<p>Now there are plenty of different ways of classifying text, this isn&rsquo;t an exhaustive list but it&rsquo;s a pretty good starting point.</p>
<p><strong>TF-IDF</strong></p>
<p>See my other two posts on TF-IDF here:</p>
<ul>
<li><a href="/tutorials/2014/12/21/tf-idf-explained-in-python.html">TF-IDF explained.</a></li>
<li><a href="/tutorials/2014/12/22/cosine-similarity-explained-in-python.html">TF-IDF and Cosine Similarity explained.</a></li>
</ul>
<p><strong>Naive Bayes Classifiers</strong></p>
<p>This is a simple Naive Bayes classifier.</p>
<pre tabindex="0"><code>from sklearn.naive\_bayes import MultinomialNB

cl = nltk.NaiveBayesClassifier.train(train\_set)
print &#34;%.3f&#34; % nltk.classify.accuracy(cl, test\_set)
cl.show\_most\_informative\_features(40)
cl.prob\_classify(featurize(name)) # get a confidence for the prediction
</code></pre><p><strong>SVC Classifier</strong></p>
<p>SVMs need numerican inputs, it can take text-based features so you have to convert these features into numbers before passing them to this classifier.</p>
<pre tabindex="0"><code>from nltk.classify import SklearnClassifier
from sklearn.svm import SVC
svmc = SklearnClassifier(SVC(), sparse=False).train(train\_features)
</code></pre><p><strong>Decision Tree Classification</strong></p>
<p>This is a simple decision tree classifier.</p>
<pre tabindex="0"><code>dtc = nltk.classify.DecisionTreeClassifier.train(train\_features, entropy\_cutoff=0, support\_cutoff=0)  
</code></pre><p><strong>Maximum Entropy Classifier</strong></p>
<p>A maximum entropy classifier and <a href="http://www.monlp.com/2011/10/10/support-for-scipy-in-nltks-maximum-entropy-methods/" target="_blank">some helpful explainers here</a>.</p>
<pre tabindex="0"><code>import numpy
import scipy

from nltk.classify import maxent
nltk.classify.MaxentClassifier.ALGORITHMS
# [&#39;GIS&#39;,&#39;IIS&#39;,&#39;CG&#39;,&#39;BFGS&#39;,&#39;Powell&#39;,&#39;LBFGSB&#39;,&#39;Nelder-Mead&#39;,&#39;MEGAM&#39;,&#39;TADM&#39;]

# MEGAM or TADM are not rec&#39;d for text classification
mec = nltk.classify.MaxentClassifier.train(train\_features, &#39;GIS&#39;, trace=0, max\_iter=1000)
</code></pre><h4 id="cross-validating-classifiers">Cross Validating Classifiers</h4>
<p>One thing you&rsquo;ll need to avoid over-fitting is you&rsquo;ll want to cross validate with k-folds. This can help you see where you might be over-fitting in your corpus.</p>
<pre tabindex="0"><code>from sklearn import cross\_validation
cv = cross\_validation.KFold(len(train\_features), n\_folds=10, indices=True, shuffle=False, random\_state=None)

for traincv, evalcv in cv:
    classifier = nltk.NaiveBayesClassifier.train(train\_features[traincv[0]:traincv[len(traincv)-1]])
    print &#39;accuracy: %.3f&#39; % nltk.classify.util.accuracy(classifier, train\_features[evalcv[0]:evalcv[len(evalcv)-1]])
</code></pre><h4 id="creating-pipelines-for-classifiers">Creating Pipelines for Classifiers</h4>
<p>Finally creating pipelines can help speed things up immensely, especially when you&rsquo;re moving to more production level code.</p>
<pre tabindex="0"><code>import sklearn
from sklearn.svm import LinearSVC
from nltk.classify.scikitlearn import SklearnClassifier
from sklearn.feature\_extraction.text import TfidfTransformer
from sklearn.feature\_selection import SelectKBest, chi2
from sklearn.naive\_bayes import MultinomialNB
from sklearn.pipeline import Pipeline
pipeline = Pipeline([(&#39;tfidf&#39;, TfidfTransformer()),
                     (&#39;chi2&#39;, SelectKBest(chi2, k=2000)),
                     (&#39;nb&#39;, MultinomialNB())])
pipecl = SklearnClassifier(pipeline)
pipecl.train(train\_features)
</code></pre>
    </div>
</article>




            </div>
            <aside class="col-12 col-md-3 float-left sidebar">
    
    <div class="sidebar-item sidebar-pages">
        <h3>Pages</h3>
        <ul>
            
            <li>
                <a href="/">Home</a>
            </li>
            
            <li>
                <a href="/about/">About</a>
            </li>
            
            <li>
                <a href="/products/">Products &amp; Projects</a>
            </li>
            
            <li>
                <a href="/posts/">Posts</a>
            </li>
            
            <li>
                <a href="/categories/">Categories</a>
            </li>
            
            <li>
                <a href="/quotes/">Quotes</a>
            </li>
            
        </ul>
    </div>
    
    <div class="sidebar-item sidebar-links">
        <h3>Links</h3>
        <ul>
            
            <li>
                <a href="" target="_blank"><span>GitHub</span></a>
            </li>
            
            <li>
                <a href="https://www.linkedin.com/in/wachambers/" target="_blank"><span>LinkedIn</span></a>
            </li>
            
            <li>
                <a href="https://twitter.com/bllchmbrs" target="_blank"><span>Twitter</span></a>
            </li>
            
        </ul>
    </div>
    
    <div class="sidebar-item sidebar-tags">
        <h3>Tags</h3>
        <div>
            
            <span>
                <a href="/tags/privacy/">privacy</a>
            </span>
            
        </div>
    </div>
    <div class="sidebar-item sidebar-toc">
        <h3>Table of Contents</h3><nav id="TableOfContents">
  <ul>
    <li>
      <ul>
        <li></li>
      </ul>
    </li>
  </ul>
</nav></div>
</aside>

        </div>
        <div class="btn">
    <div class="btn-menu" id="btn-menu">
        <i class="iconfont icon-grid-sharp"></i>
    </div>
    <div class="btn-toggle-mode">
        <i class="iconfont icon-contrast-sharp"></i>
    </div>
    <div class="btn-scroll-top">
        <i class="iconfont icon-chevron-up-circle-sharp"></i>
    </div>
</div>
<aside class="sidebar-mobile" style="display: none;">
  <div class="sidebar-wrapper">
    
    <div class="sidebar-item sidebar-pages">
        <h3>Pages</h3>
        <ul>
            
            <li>
                <a href="/">Home</a>
            </li>
            
            <li>
                <a href="/about/">About</a>
            </li>
            
            <li>
                <a href="/products/">Products &amp; Projects</a>
            </li>
            
            <li>
                <a href="/posts/">Posts</a>
            </li>
            
            <li>
                <a href="/categories/">Categories</a>
            </li>
            
            <li>
                <a href="/quotes/">Quotes</a>
            </li>
            
        </ul>
    </div>
    
    <div class="sidebar-item sidebar-links">
        <h3>Links</h3>
        <ul>
            
            <li>
                <a href="" target="_blank"><span>GitHub</span></a>
            </li>
            
            <li>
                <a href="https://www.linkedin.com/in/wachambers/" target="_blank"><span>LinkedIn</span></a>
            </li>
            
            <li>
                <a href="https://twitter.com/bllchmbrs" target="_blank"><span>Twitter</span></a>
            </li>
            
        </ul>
    </div>
    
    <div class="sidebar-item sidebar-tags">
        <h3>Tags</h3>
        <div>
            
            <span>
                <a href="/tags/privacy/">privacy</a>
            </span>
            
        </div>
    </div>
    
    
    
    
  </div>
</aside>
    </main>

    <footer>
    <div class="container-lg clearfix">
        <div class="col-12 footer">
            
            <span>&copy; 2023
                <a href="http://billchambers.me">Bill Chambers</a>
                
                | Powered by <a href="https://github.com/dsrkafuu/hugo-theme-fuji/"
                   target="_blank">Fuji-v2</a> &amp; <a href="https://gohugo.io/"
                                                    target="_blank">Hugo</a> 
            </span>
        </div>
    </div>
</footer>

    
<script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha512-N9IJRoc3LaP3NDoiGkcPa4gG94kapGpaA5Zq9/Dr04uf5TbLFU5q0o8AbRhLKUUlp8QFS2u7S+Yti0U7QtuZvQ==" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/lazysizes@5.3.2/lazysizes.min.js" integrity="sha512-q583ppKrCRc7N5O0n2nzUiJ+suUv7Et1JGels4bXOaMFQcamPk9HjdUknZuuFjBNs7tsMuadge5k9RzdmO+1GQ==" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/prismjs@1.27.0/components/prism-core.min.js" integrity="sha512-LCKPTo0gtJ74zCNMbWw04ltmujpzSR4oW+fgN+Y1YclhM5ZrHCZQAJE4quEodcI/G122sRhSGU2BsSRUZ2Gu3w==" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/prismjs@1.27.0/plugins/autoloader/prism-autoloader.min.js" integrity="sha512-GP4x8UWxWyh4BMbyJGOGneiTbkrWEF5izsVJByzVLodP8CuJH/n936+yQDMJJrOPUHLgyPbLiGw2rXmdvGdXHA==" crossorigin="anonymous"></script>



<script defer src="/assets/js/fuji.min.645f1123be695831f419ab54c1bcba327325895c740014006e57070d4f3e5d6b553e929c4b46f40ea707249e9c7f7c2a446d32a39ce7319f80a34525586a8e0f.js" integrity="sha512-ZF8RI75pWDH0GatUwby6MnMliVx0ABQAblcHDU8&#43;XWtVPpKcS0b0DqcHJJ6cf3wqRG0yo5znMZ&#43;Ao0UlWGqODw=="></script>



</body>

</html>
