<!DOCTYPE html>
<html lang="en">

<head>
    
<meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
<meta name="HandheldFriendly" content="True" />
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
<meta name="generator" content="Hugo 0.112.3">


<link rel="shortcut icon" href="https://cdn.jsdelivr.net/gh/dsrkafuu/dsr-cdn-main@1/images/favicons/dsrca.ico" />



<title>Basic Statistical NLP Part 2 - TF-IDF And Cosine Similarity - My Writings - Bill Chambers</title>


<meta name="author" content="Bill Chambers" />


<meta name="description" content="something" />



<meta property="og:title" content="Basic Statistical NLP Part 2 - TF-IDF And Cosine Similarity" />
<meta name="twitter:title" content="Basic Statistical NLP Part 2 - TF-IDF And Cosine Similarity" />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://billchambers.me/posts/cosine-similarity-explained-in-python/" /><meta property="og:description" content="This is a two part post, you can see part 1 here. Please read that post (if you haven&rsquo;t already) before continuing or just check out the code in this gist.
from **future** import division import string import math tokenize = lambda doc: doc.lower().split(&quot; &ldquo;)
document0 = &ldquo;China has a strong economy that is growing at a rapid pace. However politically it differs greatly from the US Economy.&rdquo; document1 = &ldquo;At last, China seems serious about confronting an endemic problem: domestic violence and corruption." />
<meta name="twitter:description" content="This is a two part post, you can see part 1 here. Please read that post (if you haven&rsquo;t already) before continuing or just check out the code in this gist.
from **future** import division import string import math tokenize = lambda doc: doc.lower().split(&quot; &ldquo;)
document0 = &ldquo;China has a strong economy that is growing at a rapid pace. However politically it differs greatly from the US Economy.&rdquo; document1 = &ldquo;At last, China seems serious about confronting an endemic problem: domestic violence and corruption." /><meta name="twitter:card" content="summary" /><meta property="article:published_time" content="2014-12-22T00:00:00+00:00" /><meta property="article:modified_time" content="2014-12-22T00:00:00+00:00" />


<style>
    @media (prefers-color-scheme: dark) {
        body[data-theme='auto'] img {
            filter: brightness(60%);
        }
    }

    body[data-theme='dark'] img {
        filter: brightness(60%);
    }
</style>




<link rel="stylesheet" href="http://billchambers.me/assets/css/fuji.min.b4a21b5d3eb1d0a51297e31230a65fc25e387843e45ec3a2d9176cd8d163c216d99b9b13a618b28f537c3b559ec8a408183b0fbfad48daddb9befa7d3ef90eed.css" integrity="sha512-tKIbXT6x0KUSl&#43;MSMKZfwl44eEPkXsOi2Rds2NFjwhbZm5sTphiyj1N8O1WeyKQIGDsPv61I2t25vvp9PvkO7Q==" />








</head>

<body
  data-theme="auto"
  data-theme-auto='false'
  >
    <script data-cfasync="false">
  
  var fujiThemeData = localStorage.getItem('fuji_data-theme');
  
  if (!fujiThemeData) {
    localStorage.setItem('fuji_data-theme', 'auto');
  } else {
    
    if (fujiThemeData !== 'auto') {
      document.body.setAttribute('data-theme', fujiThemeData === 'dark' ? 'dark' : 'light');
    }
  }
</script>

    <header>
    <div class="container-lg clearfix">
        <div class="col-12 header">
            <a class="title-main" href="http://billchambers.me">My Writings - Bill Chambers</a>
            
        </div>
    </div>
</header>

    <main>
        <div class="container-lg clearfix">
            
            <div class="col-12 col-md-9 float-left content">
                
<article>
    
    <h2 class="post-item post-title">
        <a href="http://billchambers.me/posts/cosine-similarity-explained-in-python/">Basic Statistical NLP Part 2 - TF-IDF And Cosine Similarity</a>
    </h2>
    <div class="post-item post-meta">
        <span><i class="iconfont icon-today-sharp"></i>&nbsp;2014-12-22</span>



<span><i class="iconfont icon-pricetags-sharp"></i>&nbsp;No tag</span>

    </div>
    
    <div class="post-content markdown-body">
        <p><em><a href="/tutorials/2014/12/21/tf-idf-explained-in-python.html">This is a two part post, you can see part 1 here.</a> Please read that post (if you haven&rsquo;t already) before continuing or just check out the code in this gist.</em></p>
<pre tabindex="0"><code>from **future** import division
import string
import math
</code></pre><p>tokenize = lambda doc: doc.lower().split(&quot; &ldquo;)</p>
<p>document<em>0 = &ldquo;China has a strong economy that is growing at a rapid pace. However politically it differs greatly from the US Economy.&rdquo;
document</em>1 = &ldquo;At last, China seems serious about confronting an endemic problem: domestic violence and corruption.&rdquo;
document<em>2 = &ldquo;Japan&rsquo;s prime minister, Shinzo Abe, is working towards healing the economic turmoil in his own country for his view on the future of his people.&rdquo;
document</em>3 = &ldquo;Vladimir Putin is working hard to fix the economy in Russia as the Ruble has tumbled.&rdquo;
document<em>4 = &ldquo;What&rsquo;s the future of Abenomics? We asked Shinzo Abe for his views&rdquo;
document</em>5 = &ldquo;Obama has eased sanctions on Cuba while accelerating those against the Russian Economy, even as the Ruble&rsquo;s value falls almost daily.&rdquo;
document_6 = &ldquo;Vladimir Putin is riding a horse while hunting deer. Vladimir Putin always seems so serious about things - even riding horses. Is he crazy?&rdquo;</p>
<p>all<em>documents = [document</em>0, document<em>1, document</em>2, document<em>3, document</em>4, document<em>5, document</em>6]</p>
<p>def jaccard_similarity(query, document):
intersection = set(query).intersection(set(document))
union = set(query).union(set(document))
return len(intersection)/len(union)</p>
<p>def term<em>frequency(term, tokenized</em>document):
return tokenized_document.count(term)</p>
<p>def sublinear<em>term</em>frequency(term, tokenized<em>document):
count = tokenized</em>document.count(term)
if count == 0:
return 0
return 1 + math.log(count)</p>
<p>def augmented<em>term</em>frequency(term, tokenized<em>document):
max</em>count = max([term<em>frequency(t, tokenized</em>document) for t in tokenized<em>document])
return (0.5 + ((0.5 * term</em>frequency(term, tokenized<em>document))/max</em>count))</p>
<p>def inverse<em>document</em>frequencies(tokenized<em>documents):
idf</em>values = {}
all<em>tokens</em>set = set([item for sublist in tokenized<em>documents for item in sublist])
for tkn in all</em>tokens<em>set:
contains</em>token = map(lambda doc: tkn in doc, tokenized<em>documents)
idf</em>values[tkn] = 1 + math.log(len(tokenized<em>documents)/(sum(contains</em>token)))
return idf_values</p>
<p>def tfidf(documents):
tokenized<em>documents = [tokenize(d) for d in documents]
idf = inverse</em>document<em>frequencies(tokenized</em>documents)
tfidf<em>documents = []
for document in tokenized</em>documents:
doc<em>tfidf = []
for term in idf.keys():
tf = sublinear</em>term<em>frequency(term, document)
doc</em>tfidf.append(tf * idf[term])
tfidf<em>documents.append(doc</em>tfidf)
return tfidf_documents</p>
<h1 id="in-scikit-learn">in Scikit-Learn</h1>
<p>from sklearn.feature_extraction.text import TfidfVectorizer</p>
<p>sklearn<em>tfidf = TfidfVectorizer(norm=&lsquo;l2&rsquo;,min</em>df=0, use<em>idf=True, smooth</em>idf=False, sublinear<em>tf=True, tokenizer=tokenize)
sklearn</em>representation = sklearn<em>tfidf.fit</em>transform(all_documents)</p>
<h3 id="cosine-similarity">Cosine Similarity</h3>
<hr>
<p>Now that we&rsquo;ve covered TF-IDF and how to do with our own code as well as Scikit-Learn. Let&rsquo;s take a look at how we can actually compare different documents with cosine similarity or the Euclidean dot product formula.</p>
<p>At this point our documents are represented as vectors.</p>
<pre tabindex="0"><code>print tfidf\_representation[0]
print sklearn\_representation.toarray()[0].tolist()
</code></pre><p>This will show you what they actually are.</p>
<p>With cosine similarity we can measure the similarity between two document vectors. I&rsquo;m not going to delve into the mathematical details about how this works but basically we turn each document into a line going from point X to point Y. We then compare that directionality with the second document into a line going from point V to point W. We measure how large the cosine angle is in between those representations.</p>
<p><img class="img-zoomable" src="http://upload.wikimedia.org/math/f/5/b/f5bc23b26d095a4040d25dd340554f5d.png" alt="Euclidean dot product" />
</p>
<p>We can rearrange the above formula to a more implementable representation like that below.</p>
<p><img class="img-zoomable" src="http://upload.wikimedia.org/math/f/3/6/f369863aa2814d6e283f859986a1574d.png" alt="Cosine Similarity" />
</p>
<p>Now in our case, if the cosine similarity is 1, they are the same document. If it is 0, the documents share nothing. This is because term frequency cannot be negative so the angle between the two vectors cannot be greater than 90Â°.</p>
<p>Here&rsquo;s our python representation of cosine similarity of two vectors in python.</p>
<pre tabindex="0"><code>def cosine\_similarity(vector1, vector2):
    dot\_product = sum(p\*q for p,q in zip(vector1, vector2))
    magnitude = math.sqrt(sum([val\*\*2 for val in vector1])) \* math.sqrt(sum([val\*\*2 for val in vector2]))
    if not magnitude:
        return 0
    return dot\_product/magnitude
</code></pre><p>Now that we have a vector representation and a way to compare different vectors we can put it to good use. But before we do, we should add that the final benefit of cosine similarity is now all our documents are off the same length, this removes any bias we had towards longer documents (like with Jaccard Similarity).</p>
<pre tabindex="0"><code>tfidf\_representation = tfidf(all\_documents)
our\_tfidf\_comparisons = []
for count\_0, doc\_0 in enumerate(tfidf\_representation):
    for count\_1, doc\_1 in enumerate(tfidf\_representation):
        our\_tfidf\_comparisons.append((cosine\_similarity(doc\_0, doc\_1), count\_0, count\_1))

skl\_tfidf\_comparisons = []
for count\_0, doc\_0 in enumerate(sklearn\_representation.toarray()):
    for count\_1, doc\_1 in enumerate(sklearn\_representation.toarray()):
        skl\_tfidf\_comparisons.append((cosine\_similarity(doc\_0, doc\_1), count\_0, count\_1))

for x in zip(sorted(our\_tfidf\_comparisons, reverse = True), sorted(skl\_tfidf\_comparisons, reverse = True)):
    print x

# ((1.0000000000000002, 4, 4), (1.0000000000000002, 6, 6))
# ((1.0000000000000002, 3, 3), (1.0000000000000002, 5, 5))
# ...
# ((0.2931092569884059, 4, 2), (0.29310925698840595, 4, 2))
# ((0.2931092569884059, 2, 4), (0.29310925698840595, 2, 4))
# ((0.16506306906464613, 6, 3), (0.16506306906464616, 6, 3))
# ...
# ((0.0, 1, 4), (0.0, 1, 4))
# ((0.0, 1, 3), (0.0, 1, 3))
# ((0.0, 1, 2), (0.0, 1, 2))
# ignore the .00000...2, it&#39;s just float representations in python being off of what they should be
</code></pre><p>What is interesting here is that we&rsquo;re getting the exact same similarities from our representation as well as Scikit-Learn&rsquo;s. This is because we&rsquo;re doing apples to apples comparisons (they are creating the vectors through the same way). We say that even though the TFIDF matrices generated by our different algorithms were different. It doesn&rsquo;t change anything down the road because vectors are just numerical representations of our sentences.</p>
<p>Now you&rsquo;ve learned a lot so far. We&rsquo;ve created some vectors to better understand document vector representations, we&rsquo;ve compared them and seen how we can leverage that representation to take a look at similarity. With a little tweaking you could even create a simple search engine that would search documents for certain terms.</p>
<p>Now there are plenty of places for improvement, better tokenization and <a href="https://pypi.python.org/pypi/PyStemmer/" target="_blank">stemming</a> could really help to improve our algorithms but that&rsquo;s for another post.</p>
<p>Here is all the code I used for this post series.</p>
<pre tabindex="0"><code>from **future** import division
import string
import math
</code></pre><p>tokenize = lambda doc: doc.lower().split(&rdquo; &ldquo;)</p>
<p>document<em>0 = &ldquo;China has a strong economy that is growing at a rapid pace. However politically it differs greatly from the US Economy.&rdquo;
document</em>1 = &ldquo;At last, China seems serious about confronting an endemic problem: domestic violence and corruption.&rdquo;
document<em>2 = &ldquo;Japan&rsquo;s prime minister, Shinzo Abe, is working towards healing the economic turmoil in his own country for his view on the future of his people.&rdquo;
document</em>3 = &ldquo;Vladimir Putin is working hard to fix the economy in Russia as the Ruble has tumbled.&rdquo;
document<em>4 = &ldquo;What&rsquo;s the future of Abenomics? We asked Shinzo Abe for his views&rdquo;
document</em>5 = &ldquo;Obama has eased sanctions on Cuba while accelerating those against the Russian Economy, even as the Ruble&rsquo;s value falls almost daily.&rdquo;
document_6 = &ldquo;Vladimir Putin is riding a horse while hunting deer. Vladimir Putin always seems so serious about things - even riding horses. Is he crazy?&rdquo;</p>
<p>all<em>documents = [document</em>0, document<em>1, document</em>2, document<em>3, document</em>4, document<em>5, document</em>6]</p>
<p>def jaccard_similarity(query, document):
intersection = set(query).intersection(set(document))
union = set(query).union(set(document))
return len(intersection)/len(union)</p>
<p>def term<em>frequency(term, tokenized</em>document):
return tokenized_document.count(term)</p>
<p>def sublinear<em>term</em>frequency(term, tokenized<em>document):
count = tokenized</em>document.count(term)
if count == 0:
return 0
return 1 + math.log(count)</p>
<p>def augmented<em>term</em>frequency(term, tokenized<em>document):
max</em>count = max([term<em>frequency(t, tokenized</em>document) for t in tokenized<em>document])
return (0.5 + ((0.5 * term</em>frequency(term, tokenized<em>document))/max</em>count))</p>
<p>def inverse<em>document</em>frequencies(tokenized<em>documents):
idf</em>values = {}
all<em>tokens</em>set = set([item for sublist in tokenized<em>documents for item in sublist])
for tkn in all</em>tokens<em>set:
contains</em>token = map(lambda doc: tkn in doc, tokenized<em>documents)
idf</em>values[tkn] = 1 + math.log(len(tokenized<em>documents)/(sum(contains</em>token)))
return idf_values</p>
<p>def tfidf(documents):
tokenized<em>documents = [tokenize(d) for d in documents]
idf = inverse</em>document<em>frequencies(tokenized</em>documents)
tfidf<em>documents = []
for document in tokenized</em>documents:
doc<em>tfidf = []
for term in idf.keys():
tf = sublinear</em>term<em>frequency(term, document)
doc</em>tfidf.append(tf * idf[term])
tfidf<em>documents.append(doc</em>tfidf)
return tfidf_documents</p>
<h1 id="in-scikit-learn-1">in Scikit-Learn</h1>
<p>from sklearn.feature_extraction.text import TfidfVectorizer</p>
<p>sklearn<em>tfidf = TfidfVectorizer(norm=&lsquo;l2&rsquo;,min</em>df=0, use<em>idf=True, smooth</em>idf=False, sublinear<em>tf=True, tokenizer=tokenize)
sklearn</em>representation = sklearn<em>tfidf.fit</em>transform(all_documents)</p>
<h6 id="-end-blog-post-1">##### END BLOG POST 1</h6>
<p>def cosine<em>similarity(vector1, vector2):
dot</em>product = sum(p<em>q for p,q in zip(vector1, vector2))
magnitude = math.sqrt(sum([val**2 for val in vector1])) * math.sqrt(sum([val</em>*2 for val in vector2]))
if not magnitude:
return 0
return dot_product/magnitude</p>
<p>tfidf<em>representation = tfidf(all</em>documents)
our<em>tfidf</em>comparisons = []
for count<em>0, doc</em>0 in enumerate(tfidf<em>representation):
for count</em>1, doc<em>1 in enumerate(tfidf</em>representation):
our<em>tfidf</em>comparisons.append((cosine<em>similarity(doc</em>0, doc<em>1), count</em>0, count_1))</p>
<p>skl<em>tfidf</em>comparisons = []
for count<em>0, doc</em>0 in enumerate(sklearn<em>representation.toarray()):
for count</em>1, doc<em>1 in enumerate(sklearn</em>representation.toarray()):
skl<em>tfidf</em>comparisons.append((cosine<em>similarity(doc</em>0, doc<em>1), count</em>0, count_1))</p>
<p>for x in zip(sorted(our<em>tfidf</em>comparisons, reverse = True), sorted(skl<em>tfidf</em>comparisons, reverse = True)):
print x</p>
<p>For further reference:</p>
<ul>
<li><a href="http://en.wikipedia.org/wiki/Cosine_similarity" target="_blank">Cosine Similarity</a></li>
</ul>

    </div>
</article>




            </div>
            <aside class="col-12 col-md-3 float-left sidebar">
    
    <div class="sidebar-item sidebar-pages">
        <h3>Pages</h3>
        <ul>
            
            <li>
                <a href="/">Home</a>
            </li>
            
            <li>
                <a href="/about/">About</a>
            </li>
            
            <li>
                <a href="/products/">Products &amp; Projects</a>
            </li>
            
            <li>
                <a href="/posts/">Posts</a>
            </li>
            
            <li>
                <a href="/categories/">Categories</a>
            </li>
            
            <li>
                <a href="/quotes/">Quotes</a>
            </li>
            
        </ul>
    </div>
    
    <div class="sidebar-item sidebar-links">
        <h3>Links</h3>
        <ul>
            
            <li>
                <a href="" target="_blank"><span>GitHub</span></a>
            </li>
            
            <li>
                <a href="https://www.linkedin.com/in/wachambers/" target="_blank"><span>LinkedIn</span></a>
            </li>
            
            <li>
                <a href="https://twitter.com/bllchmbrs" target="_blank"><span>Twitter</span></a>
            </li>
            
        </ul>
    </div>
    
    <div class="sidebar-item sidebar-tags">
        <h3>Tags</h3>
        <div>
            
            <span>
                <a href="/tags/privacy/">privacy</a>
            </span>
            
        </div>
    </div>
    <div class="sidebar-item sidebar-toc">
        <h3>Table of Contents</h3><nav id="TableOfContents">
  <ul>
    <li>
      <ul>
        <li><a href="#cosine-similarity">Cosine Similarity</a></li>
      </ul>
    </li>
  </ul>

  <ul>
    <li>
      <ul>
        <li></li>
      </ul>
    </li>
  </ul>
</nav></div>
</aside>

        </div>
        <div class="btn">
    <div class="btn-menu" id="btn-menu">
        <i class="iconfont icon-grid-sharp"></i>
    </div>
    <div class="btn-toggle-mode">
        <i class="iconfont icon-contrast-sharp"></i>
    </div>
    <div class="btn-scroll-top">
        <i class="iconfont icon-chevron-up-circle-sharp"></i>
    </div>
</div>
<aside class="sidebar-mobile" style="display: none;">
  <div class="sidebar-wrapper">
    
    <div class="sidebar-item sidebar-pages">
        <h3>Pages</h3>
        <ul>
            
            <li>
                <a href="/">Home</a>
            </li>
            
            <li>
                <a href="/about/">About</a>
            </li>
            
            <li>
                <a href="/products/">Products &amp; Projects</a>
            </li>
            
            <li>
                <a href="/posts/">Posts</a>
            </li>
            
            <li>
                <a href="/categories/">Categories</a>
            </li>
            
            <li>
                <a href="/quotes/">Quotes</a>
            </li>
            
        </ul>
    </div>
    
    <div class="sidebar-item sidebar-links">
        <h3>Links</h3>
        <ul>
            
            <li>
                <a href="" target="_blank"><span>GitHub</span></a>
            </li>
            
            <li>
                <a href="https://www.linkedin.com/in/wachambers/" target="_blank"><span>LinkedIn</span></a>
            </li>
            
            <li>
                <a href="https://twitter.com/bllchmbrs" target="_blank"><span>Twitter</span></a>
            </li>
            
        </ul>
    </div>
    
    <div class="sidebar-item sidebar-tags">
        <h3>Tags</h3>
        <div>
            
            <span>
                <a href="/tags/privacy/">privacy</a>
            </span>
            
        </div>
    </div>
    
    
    
    
  </div>
</aside>
    </main>

    <footer>
    <div class="container-lg clearfix">
        <div class="col-12 footer">
            
            <span>&copy; 2023
                <a href="http://billchambers.me">Bill Chambers</a>
                
                | Powered by <a href="https://github.com/dsrkafuu/hugo-theme-fuji/"
                   target="_blank">Fuji-v2</a> &amp; <a href="https://gohugo.io/"
                                                    target="_blank">Hugo</a> 
            </span>
        </div>
    </div>
</footer>

    
<script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha512-N9IJRoc3LaP3NDoiGkcPa4gG94kapGpaA5Zq9/Dr04uf5TbLFU5q0o8AbRhLKUUlp8QFS2u7S+Yti0U7QtuZvQ==" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/lazysizes@5.3.2/lazysizes.min.js" integrity="sha512-q583ppKrCRc7N5O0n2nzUiJ+suUv7Et1JGels4bXOaMFQcamPk9HjdUknZuuFjBNs7tsMuadge5k9RzdmO+1GQ==" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/prismjs@1.27.0/components/prism-core.min.js" integrity="sha512-LCKPTo0gtJ74zCNMbWw04ltmujpzSR4oW+fgN+Y1YclhM5ZrHCZQAJE4quEodcI/G122sRhSGU2BsSRUZ2Gu3w==" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/prismjs@1.27.0/plugins/autoloader/prism-autoloader.min.js" integrity="sha512-GP4x8UWxWyh4BMbyJGOGneiTbkrWEF5izsVJByzVLodP8CuJH/n936+yQDMJJrOPUHLgyPbLiGw2rXmdvGdXHA==" crossorigin="anonymous"></script>



<script defer src="/assets/js/fuji.min.645f1123be695831f419ab54c1bcba327325895c740014006e57070d4f3e5d6b553e929c4b46f40ea707249e9c7f7c2a446d32a39ce7319f80a34525586a8e0f.js" integrity="sha512-ZF8RI75pWDH0GatUwby6MnMliVx0ABQAblcHDU8&#43;XWtVPpKcS0b0DqcHJJ6cf3wqRG0yo5znMZ&#43;Ao0UlWGqODw=="></script>



</body>

</html>
