<!DOCTYPE html>
<html lang="en">

<head>
    
<meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
<meta name="HandheldFriendly" content="True" />
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
<meta name="generator" content="Hugo 0.112.3">


<link rel="shortcut icon" href="https://cdn.jsdelivr.net/gh/dsrkafuu/dsr-cdn-main@1/images/favicons/dsrca.ico" />



<title>Getting Started with Apache Spark - My Writings - Bill Chambers</title>


<meta name="author" content="Bill Chambers" />


<meta name="description" content="something" />



<meta property="og:title" content="Getting Started with Apache Spark" />
<meta name="twitter:title" content="Getting Started with Apache Spark" />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://billchambers.me/posts/getting-started-with-apache-spark/" /><meta property="og:description" content="Lately I&rsquo;ve been playing around with Spark for data processing. It provides some really amazing features like MLLib and Spark SQL and there&rsquo;s no better way to learn something that to use it. I&rsquo;ve attended a couple of meet ups about Spark and its related tools including the famous ampcamp put on by the developers of spark and, although I&rsquo;m not an expert, I thought it would be good to consolidate my knowledge and teach others." />
<meta name="twitter:description" content="Lately I&rsquo;ve been playing around with Spark for data processing. It provides some really amazing features like MLLib and Spark SQL and there&rsquo;s no better way to learn something that to use it. I&rsquo;ve attended a couple of meet ups about Spark and its related tools including the famous ampcamp put on by the developers of spark and, although I&rsquo;m not an expert, I thought it would be good to consolidate my knowledge and teach others." /><meta name="twitter:card" content="summary" /><meta property="article:published_time" content="2015-12-06T00:00:00+00:00" /><meta property="article:modified_time" content="2015-12-06T00:00:00+00:00" />


<style>
    @media (prefers-color-scheme: dark) {
        body[data-theme='auto'] img {
            filter: brightness(60%);
        }
    }

    body[data-theme='dark'] img {
        filter: brightness(60%);
    }
</style>




<link rel="stylesheet" href="http://billchambers.me/assets/css/fuji.min.b4a21b5d3eb1d0a51297e31230a65fc25e387843e45ec3a2d9176cd8d163c216d99b9b13a618b28f537c3b559ec8a408183b0fbfad48daddb9befa7d3ef90eed.css" integrity="sha512-tKIbXT6x0KUSl&#43;MSMKZfwl44eEPkXsOi2Rds2NFjwhbZm5sTphiyj1N8O1WeyKQIGDsPv61I2t25vvp9PvkO7Q==" />








</head>

<body
  data-theme="auto"
  data-theme-auto='false'
  >
    <script data-cfasync="false">
  
  var fujiThemeData = localStorage.getItem('fuji_data-theme');
  
  if (!fujiThemeData) {
    localStorage.setItem('fuji_data-theme', 'auto');
  } else {
    
    if (fujiThemeData !== 'auto') {
      document.body.setAttribute('data-theme', fujiThemeData === 'dark' ? 'dark' : 'light');
    }
  }
</script>

    <header>
    <div class="container-lg clearfix">
        <div class="col-12 header">
            <a class="title-main" href="http://billchambers.me">My Writings - Bill Chambers</a>
            
        </div>
    </div>
</header>

    <main>
        <div class="container-lg clearfix">
            
            <div class="col-12 col-md-9 float-left content">
                
<article>
    
    <h2 class="post-item post-title">
        <a href="http://billchambers.me/posts/getting-started-with-apache-spark/">Getting Started with Apache Spark</a>
    </h2>
    <div class="post-item post-meta">
        <span><i class="iconfont icon-today-sharp"></i>&nbsp;2015-12-06</span>



<span><i class="iconfont icon-pricetags-sharp"></i>&nbsp;No tag</span>

    </div>
    
    <div class="post-content markdown-body">
        <p>Lately I&rsquo;ve been playing around with <a href="https://spark.apache.org/" target="_blank">Spark</a> for data processing. It provides some really amazing features like <a href="https://spark.apache.org/mllib/" target="_blank">MLLib</a> and <a href="https://spark.apache.org/sql/" target="_blank">Spark SQL</a> and there&rsquo;s no better way to learn something that to use it. I&rsquo;ve attended a couple of meet ups about Spark and its related tools including the famous <a href="http://ampcamp.berkeley.edu/" target="_blank">ampcamp</a> put on by the developers of spark and, although I&rsquo;m not an expert, I thought it would be good to consolidate my knowledge and teach others.</p>
<h2 id="the-problem-spark-solves">The Problem Spark Solves</h2>
<p>I always like to best understand the problem that is being solved when I approach new tools. To throw some buzzwords around, you&rsquo;re looking to perform some data science on some big data. More simply, you&rsquo;ve got a ton of information that you want to understand more about and doing it on your computer would just be too slow.</p>
<p>Some great examples of data problems that are solved well by a tool like Apache Spark include:</p>
<ol>
<li>Analyzing Log Data
<ul>
<li>In order to hunt down a bug happening on a production server(s)</li>
</ul>
</li>
<li>Massive Natural Language Processing
<ul>
<li>Finally some use for all that twitter data you&rsquo;ve been downloading&hellip;</li>
</ul>
</li>
<li>Large Scale Recommendation Systems or General Machine Learning Tasks
<ul>
<li>Recommending products to users or trying to find related groups</li>
</ul>
</li>
</ol>
<p>While this is far from an exhaustive list, it gives a starting point to the problem we&rsquo;re solving: we&rsquo;ve got a ton of information and we want to (actionable) extract information from it.</p>
<h3 id="version-information">Version Information</h3>
<hr>
<p>In this tutorial I&rsquo;ll be using <a href="http://d3kbcqa49mib13.cloudfront.net/spark-1.1.1.tgz" target="_blank">Spark 1.1.1</a>. I&rsquo;ll also assume that you&rsquo;re on some sort of Unix system.</p>
<p>It&rsquo;s important to note that I&rsquo;ll be focusing on PySpark and you can check out the (<a href="https://spark.apache.org/docs/latest/api/python/index.html" target="_blank">documentation</a>) for the code examples.</p>
<h3 id="download">Download</h3>
<hr>
<p>Go ahead and download <a href="http://d3kbcqa49mib13.cloudfront.net/spark-1.1.1.tgz" target="_blank">Spark 1.1.1</a> right here.</p>
<p>You can download other versions of <a href="https://spark.apache.org/downloads.html" target="_blank">Apache Spark on the site.</a></p>
<p><img class="img-zoomable" src="/spark-logo.png" alt="Apache Spark" />
</p>
<h3 id="the-basics">The Basics</h3>
<hr>
<p>Once we&rsquo;ve got Spark downloaded, let&rsquo;s get started. You&rsquo;re going to have to build Spark yourself once you&rsquo;ve downloaded it but as the readme will show you, it&rsquo;s pretty straight forward. Just run <code>./sbt/sbt assembly</code> in the root download folder directory. It may take a while to build (took me 15+ minutes from scratch) but just be patient. I&rsquo;d grab a cup of coffee.</p>
<p>Once it&rsquo;s all built, you should be ready to go. Let&rsquo;s dive right into the <code>PySpark</code> shell with <code>./bin/pyspark</code> on the command line.</p>
<p>You should see some log information then boom you&rsquo;ll be in the Spark Shell.</p>
<pre tabindex="0"><code>Welcome to
      \_\_\_\_              \_\_
     / \_\_/\_\_  \_\_\_ \_\_\_\_\_/ /\_\_
    \_\ \/ \_ \/ \_ `/ \_\_/  &#39;\_/
 /\_\_ / .\_\_/\\_,\_/\_/ /\_/\\_\ version 1.1.1
 /\_/

Using Python version 2.7.6 (default, Sep 9 2014 15:04:36)
SparkContext available as sc.
&gt;&gt;&gt;
</code></pre><p>On a quick note while I love the regular python shell, I really like the <a href="http://ipython.org/" target="_blank">IPython Shell</a> much better. It&rsquo;s got tons of handy tools built right in and using IPython with PySpark is super easy. Just close out of that Python Shell and set the IPYTHON environmental variable to 1. Run this in your bash shell.</p>
<pre tabindex="0"><code>export IPYTHON=1
./bin/pyspark
</code></pre><p>Now we&rsquo;re loaded up into a python shell and we can finally get to work! Now the python shell we&rsquo;re in is the same python shell that you have on your machine. We can import libraries, do math, same &lsquo;ol stuff.</p>
<pre tabindex="0"><code>import numpy as np
1 + 1
# 2
x = 5
x
# 5
</code></pre><p>There is however, one difference, the <code>sc</code> variable or SparkContext. You can see this is what was made available when we started up the shell previously.</p>
<pre tabindex="0"><code>type(sc)
# pyspark.context.SparkContext
</code></pre><p>This spark context is the source for all of our handy Spark features.</p>
<p>Let&rsquo;s go ahead and get started analyzing some data so we can better understand the SparkContext. Let&rsquo;s start small to understand the concepts. I&rsquo;ll be using <a href="https://www.kaggle.com/c/sentiment-analysis-on-movie-reviews" target="_blank">the Rotten Tomatoes Dataset from Kaggle</a>. Go ahead and download it and put it in the same Spark download folder on your machine.</p>
<p>Now, let&rsquo;s go ahead and open it on up and get things started. We load in data with the <code>sc.textFile('file/path')</code></p>
<pre tabindex="0"><code>data = sc.textFile(&#39;../rotten\_tomatoes\_train.tsv&#39;)
type(data)
# pyspark.rdd.RDD
</code></pre><p>Our dataset is now loaded into spark as an <strong>RDD</strong> or <strong>Resilient Distributed Dataset</strong>. This is the fundamental abstraction in Spark and basically it is a representation of a dataset that is distributed through the cluster. <em>Obviously at this point we don&rsquo;t have a cluster running as we are just on our local machine but the same concepts apply.</em></p>
<p>One of the most important concepts concerning RDDs is that they are immutable. What you&rsquo;re doing is applying a series of <code>transformations</code> to data (stored in the RDD) then finally performing an <code>action</code> in order to get an answer.</p>
<p><em>Just as a note, our columns are tab separated and arranged as follows:</em></p>
<p><em>PhraseId SentenceId Phrase Sentiment</em></p>
<p>First let&rsquo;s get the count, how many items(lines) do we have in our data? This is an example of an <code>action</code></p>
<pre tabindex="0"><code>data.count()
# ...log statements
# 156061
</code></pre><p>There we get an answer because we&rsquo;re requesting an <code>action</code> as opposed to a <code>transformation</code>. Let&rsquo;s perform one of those now.</p>
<pre tabindex="0"><code>negative\_reviews = data.filter(lambda line: &#39;0&#39; == line.split(&#39;\t&#39;)[3])
print negative\_reviews
# PythonRDD[14] at RDD at PythonRDD.scala:43
print type(negative\_reviews)
# &lt;class &#39;pyspark.rdd.PipelinedRDD&#39;&gt;
</code></pre><p>Notice how when we go to print it, it prints out that it is an RDD and that the type is a <code>PipelinedRDD</code> not a list of values as we might expect. That&rsquo;s because we haven&rsquo;t performed an <code>action</code> yet, we&rsquo;ve only performed a <code>transformation</code>.</p>
<p>As a side note, we could write the above code this way as well.</p>
<pre tabindex="0"><code>negative\_reviews = data \
                    .map(lambda line: line.split(&#39;\t&#39;)) \
                    .filter(lambda line: line[3] == &#39;0&#39;)
</code></pre><p>You&rsquo;ll notice that when we run <code>transformation</code> commands, nothing gets printed out. That&rsquo;s because Spark commands are <strong>lazily evaluated</strong>. We set up a <code>transformation</code> from RDD to RDD to prepare to run an <code>action</code> to get back results.</p>
<p>Here are some examples of other <code>actions</code>:</p>
<pre tabindex="0"><code>negative\_reviews.count()
# 7072
negative\_reviews.first()
# [u&#39;102&#39;, u&#39;3&#39;, u&#39;would have a hard time sitting through this one&#39;, u&#39;0&#39;]
</code></pre><p>Now that we&rsquo;ve got a better idea about how Spark works and executes commands through <code>transformations</code> and <code>actions</code> let&rsquo;s write ourselves a little MapReduce job to figure out what words show up most commonly in the negative reviews.</p>
<pre tabindex="0"><code>word\_counts = negative\_reviews \
                .flatMap(lambda line: line[2].split()) \
                .map(lambda word: (word,1)) \
                .reduceByKey(lambda a,b: a+b)
</code></pre><p>What we&rsquo;re doing above is getting the word counts. We do that by tokenizing the review text in a <code>flatMap</code> which basically just makes it into one big giant list of words. Then we map each word to 1 and <code>reduceByKey</code> which parallelizes our reduce operation to each key in our data set.</p>
<p>The above code is equivalent to:</p>
<pre tabindex="0"><code>
word\_counts = negative\_reviews \
                .flatMap(lambda line: line[2].split()) \
                .map(lambda word: (word,1)) \
                .groupByKey() \
                .map(lambda val: (val[0], len(val[1]))) # basically the value and the generator
</code></pre><p>Now let&rsquo;s get the most common word we can find in the reviews.</p>
<pre tabindex="0"><code>def most\_common(comp1, comp2):
    word1, count1 = comp1
    word2, count2 = comp2
    if count1 &gt; count2:
        return (word1, count1)
    else:
        return (word2, count2)

most\_common\_word = word\_counts.reduce(most\_common)
print most\_common\_word
# (u&#39;,&#39;, 3722)
</code></pre><p>Knowing that it is a comma isn&rsquo;t too useful, let&rsquo;s try it without punctuation, which means rewriting our map function.</p>
<pre tabindex="0"><code>import string
def clean\_words(word):
    if word not in string.punctuation:
        return (word, 1)
    else:
        return (word, 0)

word\_counts = negative\_reviews \
                .flatMap(lambda line: line[2].split()) \
                .map(clean\_words) \
                .reduceByKey(lambda a,b: a+b)

most\_common\_word = word\_counts.reduce(most\_common)
print most\_common\_word
# (u&#39;the&#39;, 3070)
</code></pre><p>Now that&rsquo;s not particularly useful either so let&rsquo;s try getting just the top 5. We&rsquo;ll keep using the word_counts that excludes punctuation.</p>
<pre tabindex="0"><code>word\_counts.sortBy(lambda x: x[1]\*-1).take(5)
# (u&#39;the&#39;, 3070), (u&#39;a&#39;, 2572), (u&#39;and&#39;, 2507), (u&#39;of&#39;, 2236), (u&#39;to&#39;, 1880)]
</code></pre><p>Note how we had to multiply it by negative one in order to sort it least to greatest.</p>
<p>Finally now that we&rsquo;ve performed some analysis. We might want to store those <code>word_counts</code> for some later use. Speaking of which, it&rsquo;d also be valuable to store the <code>negative_reviews</code> too. It&rsquo;s helpful to imagine this being a much larger dataset as this is a trivial example but let&rsquo;s go ahead and save it.</p>
<pre tabindex="0"><code>negative\_reviews.saveAsTextFile(&#39;negative\_reviews&#39;)
word\_counts.saveAsSequenceFile(&#39;word\_counts\_negative\_reviews&#39;)
</code></pre><p>Note that negative reviews is saved as a Hadoop text file while word counts is saved as a sequence file. Because these datasets are typically large, we have to save them as these distributed data files. Here&rsquo;s a list of some of the ways you can save these files.</p>
<pre tabindex="0"><code>data.saveAsHadoopDataset()
data.saveAsNewAPIHadoopFile()
data.saveAsTextFile()
data.saveAsHadoopFile()
data.saveAsPickleFile()
data.saveAsNewAPIHadoopDataset()
data.saveAsSequenceFile()
</code></pre><p>Those are the basics for how to analyze data in Spark. In some later posts I&rsquo;ll be going over how to analyze larger sets of data. Going over how to run MapReduce jobs on Spark, using SparkSQL and more.</p>

    </div>
</article>




            </div>
            <aside class="col-12 col-md-3 float-left sidebar">
    
    <div class="sidebar-item sidebar-pages">
        <h3>Pages</h3>
        <ul>
            
            <li>
                <a href="/">Home</a>
            </li>
            
            <li>
                <a href="/about/">About</a>
            </li>
            
            <li>
                <a href="/products/">Products &amp; Projects</a>
            </li>
            
            <li>
                <a href="/posts/">Posts</a>
            </li>
            
            <li>
                <a href="/categories/">Categories</a>
            </li>
            
            <li>
                <a href="/quotes/">Quotes</a>
            </li>
            
        </ul>
    </div>
    
    <div class="sidebar-item sidebar-links">
        <h3>Links</h3>
        <ul>
            
            <li>
                <a href="" target="_blank"><span>GitHub</span></a>
            </li>
            
            <li>
                <a href="https://www.linkedin.com/in/wachambers/" target="_blank"><span>LinkedIn</span></a>
            </li>
            
            <li>
                <a href="https://twitter.com/bllchmbrs" target="_blank"><span>Twitter</span></a>
            </li>
            
        </ul>
    </div>
    
    <div class="sidebar-item sidebar-tags">
        <h3>Tags</h3>
        <div>
            
            <span>
                <a href="/tags/privacy/">privacy</a>
            </span>
            
        </div>
    </div>
    <div class="sidebar-item sidebar-toc">
        <h3>Table of Contents</h3><nav id="TableOfContents">
  <ul>
    <li><a href="#the-problem-spark-solves">The Problem Spark Solves</a>
      <ul>
        <li><a href="#version-information">Version Information</a></li>
        <li><a href="#download">Download</a></li>
        <li><a href="#the-basics">The Basics</a></li>
      </ul>
    </li>
  </ul>
</nav></div>
</aside>

        </div>
        <div class="btn">
    <div class="btn-menu" id="btn-menu">
        <i class="iconfont icon-grid-sharp"></i>
    </div>
    <div class="btn-toggle-mode">
        <i class="iconfont icon-contrast-sharp"></i>
    </div>
    <div class="btn-scroll-top">
        <i class="iconfont icon-chevron-up-circle-sharp"></i>
    </div>
</div>
<aside class="sidebar-mobile" style="display: none;">
  <div class="sidebar-wrapper">
    
    <div class="sidebar-item sidebar-pages">
        <h3>Pages</h3>
        <ul>
            
            <li>
                <a href="/">Home</a>
            </li>
            
            <li>
                <a href="/about/">About</a>
            </li>
            
            <li>
                <a href="/products/">Products &amp; Projects</a>
            </li>
            
            <li>
                <a href="/posts/">Posts</a>
            </li>
            
            <li>
                <a href="/categories/">Categories</a>
            </li>
            
            <li>
                <a href="/quotes/">Quotes</a>
            </li>
            
        </ul>
    </div>
    
    <div class="sidebar-item sidebar-links">
        <h3>Links</h3>
        <ul>
            
            <li>
                <a href="" target="_blank"><span>GitHub</span></a>
            </li>
            
            <li>
                <a href="https://www.linkedin.com/in/wachambers/" target="_blank"><span>LinkedIn</span></a>
            </li>
            
            <li>
                <a href="https://twitter.com/bllchmbrs" target="_blank"><span>Twitter</span></a>
            </li>
            
        </ul>
    </div>
    
    <div class="sidebar-item sidebar-tags">
        <h3>Tags</h3>
        <div>
            
            <span>
                <a href="/tags/privacy/">privacy</a>
            </span>
            
        </div>
    </div>
    
    
    
    
  </div>
</aside>
    </main>

    <footer>
    <div class="container-lg clearfix">
        <div class="col-12 footer">
            
            <span>&copy; 2023
                <a href="http://billchambers.me">Bill Chambers</a>
                
                | Powered by <a href="https://github.com/dsrkafuu/hugo-theme-fuji/"
                   target="_blank">Fuji-v2</a> &amp; <a href="https://gohugo.io/"
                                                    target="_blank">Hugo</a> 
            </span>
        </div>
    </div>
</footer>

    
<script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha512-N9IJRoc3LaP3NDoiGkcPa4gG94kapGpaA5Zq9/Dr04uf5TbLFU5q0o8AbRhLKUUlp8QFS2u7S+Yti0U7QtuZvQ==" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/lazysizes@5.3.2/lazysizes.min.js" integrity="sha512-q583ppKrCRc7N5O0n2nzUiJ+suUv7Et1JGels4bXOaMFQcamPk9HjdUknZuuFjBNs7tsMuadge5k9RzdmO+1GQ==" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/prismjs@1.27.0/components/prism-core.min.js" integrity="sha512-LCKPTo0gtJ74zCNMbWw04ltmujpzSR4oW+fgN+Y1YclhM5ZrHCZQAJE4quEodcI/G122sRhSGU2BsSRUZ2Gu3w==" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/prismjs@1.27.0/plugins/autoloader/prism-autoloader.min.js" integrity="sha512-GP4x8UWxWyh4BMbyJGOGneiTbkrWEF5izsVJByzVLodP8CuJH/n936+yQDMJJrOPUHLgyPbLiGw2rXmdvGdXHA==" crossorigin="anonymous"></script>



<script defer src="/assets/js/fuji.min.645f1123be695831f419ab54c1bcba327325895c740014006e57070d4f3e5d6b553e929c4b46f40ea707249e9c7f7c2a446d32a39ce7319f80a34525586a8e0f.js" integrity="sha512-ZF8RI75pWDH0GatUwby6MnMliVx0ABQAblcHDU8&#43;XWtVPpKcS0b0DqcHJJ6cf3wqRG0yo5znMZ&#43;Ao0UlWGqODw=="></script>



</body>

</html>
